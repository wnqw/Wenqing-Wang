---
title: "Audio-Driven Emotional 3D Talking-Head Generation"
date: 2024-09-30
publishDate: 2024-09-30
authors: ["Wenqing Wang", "Yun Fu"]
publication_types: ["1"]  # Use "1" for conference papers, "2" for journal articles, etc.
abstract: "Audio-driven talking-head generation is a crucial and useful technology in virtual human interaction and film- making applications. Recent advancements have focused on improving the image fidelity and lip-synchronization. How- ever, generating accurate emotional expressions is an impor- tant aspect of realistic talking-head generation, which has remained underexplored in previous works. In this paper, we present a novel framework for synthesizing high-fidelity, audio- driven video portraits with accurate emotional expressions. Specifically, we utilize a variational autoencoder (VAE)-based audio-to-motion module to generate facial landmarks. These landmarks are concatenated with emotional embeddings to produce emotional landmarks through our motion-to-emotion module. These emotional landmarks are then used to render realistic emotional talking-head video using a Neural Radiance Fields (NeRF)-based emotion-to-video module. Additionally, we propose a pose sampling method that generates natural idle- state (non-speaking) videos in response to silent audio inputs. Extensive experiments show that our method demonstrates high-fidelity emotion generation compared to previous methods."
featured: false
publication: "IEEE International Conference on Automatic Face and Gesture Recognition"
tags: ["NeRF", "Digital Humans", "emotion", "Audio-driven video portraits"]

url_pdf: https://arxiv.org/abs/2410.17262
---